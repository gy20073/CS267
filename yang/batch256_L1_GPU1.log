I0429 22:33:33.123891 16166 caffe.cpp:185] Using GPUs 0
I0429 22:33:33.201431 16166 caffe.cpp:190] GPU 0: Tesla K40c
I0429 22:33:33.710131 16166 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 200
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
batch_l: 1
I0429 22:33:33.710353 16166 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0429 22:33:33.711107 16166 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0429 22:33:33.711136 16166 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0429 22:33:33.711256 16166 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0429 22:33:33.711380 16166 layer_factory.hpp:77] Creating layer mnist
I0429 22:33:33.712303 16166 net.cpp:91] Creating Layer mnist
I0429 22:33:33.712357 16166 net.cpp:399] mnist -> data
I0429 22:33:33.712419 16166 net.cpp:399] mnist -> label
I0429 22:33:33.714429 16170 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0429 22:33:33.729532 16166 data_layer.cpp:41] output data size: 256,1,28,28
I0429 22:33:33.733914 16166 net.cpp:141] Setting up mnist
I0429 22:33:33.733949 16166 net.cpp:148] Top shape: 256 1 28 28 (200704)
I0429 22:33:33.733961 16166 net.cpp:148] Top shape: 256 (256)
I0429 22:33:33.733966 16166 net.cpp:156] Memory required for data: 803840
I0429 22:33:33.733979 16166 layer_factory.hpp:77] Creating layer conv1
I0429 22:33:33.734017 16166 net.cpp:91] Creating Layer conv1
I0429 22:33:33.734027 16166 net.cpp:425] conv1 <- data
I0429 22:33:33.734043 16166 net.cpp:399] conv1 -> conv1
I0429 22:33:33.944164 16166 net.cpp:141] Setting up conv1
I0429 22:33:33.944214 16166 net.cpp:148] Top shape: 256 20 24 24 (2949120)
I0429 22:33:33.944221 16166 net.cpp:156] Memory required for data: 12600320
I0429 22:33:33.944252 16166 layer_factory.hpp:77] Creating layer pool1
I0429 22:33:33.944279 16166 net.cpp:91] Creating Layer pool1
I0429 22:33:33.944288 16166 net.cpp:425] pool1 <- conv1
I0429 22:33:33.944370 16166 net.cpp:399] pool1 -> pool1
I0429 22:33:33.944454 16166 net.cpp:141] Setting up pool1
I0429 22:33:33.944469 16166 net.cpp:148] Top shape: 256 20 12 12 (737280)
I0429 22:33:33.944474 16166 net.cpp:156] Memory required for data: 15549440
I0429 22:33:33.944480 16166 layer_factory.hpp:77] Creating layer conv2
I0429 22:33:33.944501 16166 net.cpp:91] Creating Layer conv2
I0429 22:33:33.944507 16166 net.cpp:425] conv2 <- pool1
I0429 22:33:33.944515 16166 net.cpp:399] conv2 -> conv2
I0429 22:33:33.947002 16166 net.cpp:141] Setting up conv2
I0429 22:33:33.947024 16166 net.cpp:148] Top shape: 256 50 8 8 (819200)
I0429 22:33:33.947031 16166 net.cpp:156] Memory required for data: 18826240
I0429 22:33:33.947044 16166 layer_factory.hpp:77] Creating layer pool2
I0429 22:33:33.947055 16166 net.cpp:91] Creating Layer pool2
I0429 22:33:33.947062 16166 net.cpp:425] pool2 <- conv2
I0429 22:33:33.947069 16166 net.cpp:399] pool2 -> pool2
I0429 22:33:33.947121 16166 net.cpp:141] Setting up pool2
I0429 22:33:33.947131 16166 net.cpp:148] Top shape: 256 50 4 4 (204800)
I0429 22:33:33.947136 16166 net.cpp:156] Memory required for data: 19645440
I0429 22:33:33.947141 16166 layer_factory.hpp:77] Creating layer ip1
I0429 22:33:33.947152 16166 net.cpp:91] Creating Layer ip1
I0429 22:33:33.947157 16166 net.cpp:425] ip1 <- pool2
I0429 22:33:33.947165 16166 net.cpp:399] ip1 -> ip1
I0429 22:33:33.952507 16166 net.cpp:141] Setting up ip1
I0429 22:33:33.952527 16166 net.cpp:148] Top shape: 256 500 (128000)
I0429 22:33:33.952533 16166 net.cpp:156] Memory required for data: 20157440
I0429 22:33:33.952546 16166 layer_factory.hpp:77] Creating layer relu1
I0429 22:33:33.952561 16166 net.cpp:91] Creating Layer relu1
I0429 22:33:33.952567 16166 net.cpp:425] relu1 <- ip1
I0429 22:33:33.952575 16166 net.cpp:386] relu1 -> ip1 (in-place)
I0429 22:33:33.952841 16166 net.cpp:141] Setting up relu1
I0429 22:33:33.952862 16166 net.cpp:148] Top shape: 256 500 (128000)
I0429 22:33:33.952867 16166 net.cpp:156] Memory required for data: 20669440
I0429 22:33:33.952872 16166 layer_factory.hpp:77] Creating layer ip2
I0429 22:33:33.952883 16166 net.cpp:91] Creating Layer ip2
I0429 22:33:33.952888 16166 net.cpp:425] ip2 <- ip1
I0429 22:33:33.952898 16166 net.cpp:399] ip2 -> ip2
I0429 22:33:33.953963 16166 net.cpp:141] Setting up ip2
I0429 22:33:33.953981 16166 net.cpp:148] Top shape: 256 10 (2560)
I0429 22:33:33.953986 16166 net.cpp:156] Memory required for data: 20679680
I0429 22:33:33.953995 16166 layer_factory.hpp:77] Creating layer loss
I0429 22:33:33.954015 16166 net.cpp:91] Creating Layer loss
I0429 22:33:33.954020 16166 net.cpp:425] loss <- ip2
I0429 22:33:33.954027 16166 net.cpp:425] loss <- label
I0429 22:33:33.954036 16166 net.cpp:399] loss -> loss
I0429 22:33:33.954063 16166 layer_factory.hpp:77] Creating layer loss
I0429 22:33:33.955499 16166 net.cpp:141] Setting up loss
I0429 22:33:33.955518 16166 net.cpp:148] Top shape: (1)
I0429 22:33:33.955523 16166 net.cpp:151]     with loss weight 1
I0429 22:33:33.955556 16166 net.cpp:156] Memory required for data: 20679684
I0429 22:33:33.955562 16166 net.cpp:217] loss needs backward computation.
I0429 22:33:33.955569 16166 net.cpp:217] ip2 needs backward computation.
I0429 22:33:33.955574 16166 net.cpp:217] relu1 needs backward computation.
I0429 22:33:33.955577 16166 net.cpp:217] ip1 needs backward computation.
I0429 22:33:33.955581 16166 net.cpp:217] pool2 needs backward computation.
I0429 22:33:33.955586 16166 net.cpp:217] conv2 needs backward computation.
I0429 22:33:33.955590 16166 net.cpp:217] pool1 needs backward computation.
I0429 22:33:33.955595 16166 net.cpp:217] conv1 needs backward computation.
I0429 22:33:33.955600 16166 net.cpp:219] mnist does not need backward computation.
I0429 22:33:33.955605 16166 net.cpp:261] This network produces output loss
I0429 22:33:33.955621 16166 net.cpp:274] Network initialization done.
I0429 22:33:33.956053 16166 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0429 22:33:33.956096 16166 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0429 22:33:33.956276 16166 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0429 22:33:33.956389 16166 layer_factory.hpp:77] Creating layer mnist
I0429 22:33:33.956569 16166 net.cpp:91] Creating Layer mnist
I0429 22:33:33.956583 16166 net.cpp:399] mnist -> data
I0429 22:33:33.956595 16166 net.cpp:399] mnist -> label
I0429 22:33:33.960918 16172 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I0429 22:33:33.961122 16166 data_layer.cpp:41] output data size: 100,1,28,28
I0429 22:33:33.963932 16166 net.cpp:141] Setting up mnist
I0429 22:33:33.963949 16166 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0429 22:33:33.963956 16166 net.cpp:148] Top shape: 100 (100)
I0429 22:33:33.963961 16166 net.cpp:156] Memory required for data: 314000
I0429 22:33:33.963966 16166 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0429 22:33:33.964071 16166 net.cpp:91] Creating Layer label_mnist_1_split
I0429 22:33:33.964082 16166 net.cpp:425] label_mnist_1_split <- label
I0429 22:33:33.964093 16166 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0
I0429 22:33:33.964104 16166 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1
I0429 22:33:33.964237 16166 net.cpp:141] Setting up label_mnist_1_split
I0429 22:33:33.964251 16166 net.cpp:148] Top shape: 100 (100)
I0429 22:33:33.964257 16166 net.cpp:148] Top shape: 100 (100)
I0429 22:33:33.964262 16166 net.cpp:156] Memory required for data: 314800
I0429 22:33:33.964267 16166 layer_factory.hpp:77] Creating layer conv1
I0429 22:33:33.964292 16166 net.cpp:91] Creating Layer conv1
I0429 22:33:33.964301 16166 net.cpp:425] conv1 <- data
I0429 22:33:33.964311 16166 net.cpp:399] conv1 -> conv1
I0429 22:33:33.966954 16166 net.cpp:141] Setting up conv1
I0429 22:33:33.966980 16166 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0429 22:33:33.966986 16166 net.cpp:156] Memory required for data: 4922800
I0429 22:33:33.967003 16166 layer_factory.hpp:77] Creating layer pool1
I0429 22:33:33.967049 16166 net.cpp:91] Creating Layer pool1
I0429 22:33:33.967056 16166 net.cpp:425] pool1 <- conv1
I0429 22:33:33.967068 16166 net.cpp:399] pool1 -> pool1
I0429 22:33:33.967133 16166 net.cpp:141] Setting up pool1
I0429 22:33:33.967146 16166 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0429 22:33:33.967150 16166 net.cpp:156] Memory required for data: 6074800
I0429 22:33:33.967155 16166 layer_factory.hpp:77] Creating layer conv2
I0429 22:33:33.967176 16166 net.cpp:91] Creating Layer conv2
I0429 22:33:33.967182 16166 net.cpp:425] conv2 <- pool1
I0429 22:33:33.967190 16166 net.cpp:399] conv2 -> conv2
I0429 22:33:33.971246 16166 net.cpp:141] Setting up conv2
I0429 22:33:33.971271 16166 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0429 22:33:33.971276 16166 net.cpp:156] Memory required for data: 7354800
I0429 22:33:33.971305 16166 layer_factory.hpp:77] Creating layer pool2
I0429 22:33:33.971320 16166 net.cpp:91] Creating Layer pool2
I0429 22:33:33.971326 16166 net.cpp:425] pool2 <- conv2
I0429 22:33:33.971334 16166 net.cpp:399] pool2 -> pool2
I0429 22:33:33.971449 16166 net.cpp:141] Setting up pool2
I0429 22:33:33.971464 16166 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0429 22:33:33.971469 16166 net.cpp:156] Memory required for data: 7674800
I0429 22:33:33.971474 16166 layer_factory.hpp:77] Creating layer ip1
I0429 22:33:33.971484 16166 net.cpp:91] Creating Layer ip1
I0429 22:33:33.971489 16166 net.cpp:425] ip1 <- pool2
I0429 22:33:33.971503 16166 net.cpp:399] ip1 -> ip1
I0429 22:33:33.977469 16166 net.cpp:141] Setting up ip1
I0429 22:33:33.977489 16166 net.cpp:148] Top shape: 100 500 (50000)
I0429 22:33:33.977494 16166 net.cpp:156] Memory required for data: 7874800
I0429 22:33:33.977509 16166 layer_factory.hpp:77] Creating layer relu1
I0429 22:33:33.977517 16166 net.cpp:91] Creating Layer relu1
I0429 22:33:33.977524 16166 net.cpp:425] relu1 <- ip1
I0429 22:33:33.977530 16166 net.cpp:386] relu1 -> ip1 (in-place)
I0429 22:33:33.978409 16166 net.cpp:141] Setting up relu1
I0429 22:33:33.978436 16166 net.cpp:148] Top shape: 100 500 (50000)
I0429 22:33:33.978442 16166 net.cpp:156] Memory required for data: 8074800
I0429 22:33:33.978452 16166 layer_factory.hpp:77] Creating layer ip2
I0429 22:33:33.978477 16166 net.cpp:91] Creating Layer ip2
I0429 22:33:33.978485 16166 net.cpp:425] ip2 <- ip1
I0429 22:33:33.978502 16166 net.cpp:399] ip2 -> ip2
I0429 22:33:33.978785 16166 net.cpp:141] Setting up ip2
I0429 22:33:33.978801 16166 net.cpp:148] Top shape: 100 10 (1000)
I0429 22:33:33.978806 16166 net.cpp:156] Memory required for data: 8078800
I0429 22:33:33.978817 16166 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0429 22:33:33.978828 16166 net.cpp:91] Creating Layer ip2_ip2_0_split
I0429 22:33:33.978837 16166 net.cpp:425] ip2_ip2_0_split <- ip2
I0429 22:33:33.978849 16166 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0429 22:33:33.978859 16166 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0429 22:33:33.978916 16166 net.cpp:141] Setting up ip2_ip2_0_split
I0429 22:33:33.978929 16166 net.cpp:148] Top shape: 100 10 (1000)
I0429 22:33:33.978935 16166 net.cpp:148] Top shape: 100 10 (1000)
I0429 22:33:33.978940 16166 net.cpp:156] Memory required for data: 8086800
I0429 22:33:33.978945 16166 layer_factory.hpp:77] Creating layer accuracy
I0429 22:33:33.978960 16166 net.cpp:91] Creating Layer accuracy
I0429 22:33:33.978966 16166 net.cpp:425] accuracy <- ip2_ip2_0_split_0
I0429 22:33:33.978973 16166 net.cpp:425] accuracy <- label_mnist_1_split_0
I0429 22:33:33.978981 16166 net.cpp:399] accuracy -> accuracy
I0429 22:33:33.978999 16166 net.cpp:141] Setting up accuracy
I0429 22:33:33.979007 16166 net.cpp:148] Top shape: (1)
I0429 22:33:33.979012 16166 net.cpp:156] Memory required for data: 8086804
I0429 22:33:33.979017 16166 layer_factory.hpp:77] Creating layer loss
I0429 22:33:33.979028 16166 net.cpp:91] Creating Layer loss
I0429 22:33:33.979034 16166 net.cpp:425] loss <- ip2_ip2_0_split_1
I0429 22:33:33.979040 16166 net.cpp:425] loss <- label_mnist_1_split_1
I0429 22:33:33.979050 16166 net.cpp:399] loss -> loss
I0429 22:33:33.979117 16166 layer_factory.hpp:77] Creating layer loss
I0429 22:33:33.980288 16166 net.cpp:141] Setting up loss
I0429 22:33:33.980310 16166 net.cpp:148] Top shape: (1)
I0429 22:33:33.980315 16166 net.cpp:151]     with loss weight 1
I0429 22:33:33.980332 16166 net.cpp:156] Memory required for data: 8086808
I0429 22:33:33.980339 16166 net.cpp:217] loss needs backward computation.
I0429 22:33:33.980345 16166 net.cpp:219] accuracy does not need backward computation.
I0429 22:33:33.980350 16166 net.cpp:217] ip2_ip2_0_split needs backward computation.
I0429 22:33:33.980355 16166 net.cpp:217] ip2 needs backward computation.
I0429 22:33:33.980360 16166 net.cpp:217] relu1 needs backward computation.
I0429 22:33:33.980365 16166 net.cpp:217] ip1 needs backward computation.
I0429 22:33:33.980368 16166 net.cpp:217] pool2 needs backward computation.
I0429 22:33:33.980373 16166 net.cpp:217] conv2 needs backward computation.
I0429 22:33:33.980378 16166 net.cpp:217] pool1 needs backward computation.
I0429 22:33:33.980383 16166 net.cpp:217] conv1 needs backward computation.
I0429 22:33:33.980388 16166 net.cpp:219] label_mnist_1_split does not need backward computation.
I0429 22:33:33.980394 16166 net.cpp:219] mnist does not need backward computation.
I0429 22:33:33.980397 16166 net.cpp:261] This network produces output accuracy
I0429 22:33:33.980403 16166 net.cpp:261] This network produces output loss
I0429 22:33:33.980422 16166 net.cpp:274] Network initialization done.
I0429 22:33:33.980504 16166 solver.cpp:60] Solver scaffolding done.
I0429 22:33:33.980947 16166 caffe.cpp:219] Starting Optimization
I0429 22:33:33.980967 16166 solver.cpp:281] Solving LeNet
I0429 22:33:33.980978 16166 solver.cpp:282] Learning Rate Policy: inv
I0429 22:33:33.980991 16166 solver.cpp:339] Iteration 0, Testing net (#0)
I0429 22:33:34.144551 16166 solver.cpp:406]     Test net output #0: accuracy = 0.1403
I0429 22:33:34.144618 16166 solver.cpp:406]     Test net output #1: loss = 2.27938 (* 1 = 2.27938 loss)
I0429 22:33:34.151371 16166 solver.cpp:229] Iteration 0, loss = 2.25035
I0429 22:33:34.151410 16166 solver.cpp:245]     Train net output #0: loss = 2.25035 (* 1 = 2.25035 loss)
I0429 22:33:34.151460 16166 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0429 22:33:35.087402 16166 solver.cpp:229] Iteration 100, loss = 0.239916
I0429 22:33:35.087476 16166 solver.cpp:245]     Train net output #0: loss = 0.239916 (* 1 = 0.239916 loss)
I0429 22:33:35.087492 16166 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I0429 22:33:36.005882 16166 solver.cpp:339] Iteration 200, Testing net (#0)
I0429 22:33:36.163347 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9554
I0429 22:33:36.163403 16166 solver.cpp:406]     Test net output #1: loss = 0.149484 (* 1 = 0.149484 loss)
I0429 22:33:36.166482 16166 solver.cpp:229] Iteration 200, loss = 0.150072
I0429 22:33:36.166512 16166 solver.cpp:245]     Train net output #0: loss = 0.150072 (* 1 = 0.150072 loss)
I0429 22:33:36.166525 16166 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I0429 22:33:37.092201 16166 solver.cpp:229] Iteration 300, loss = 0.110661
I0429 22:33:37.092257 16166 solver.cpp:245]     Train net output #0: loss = 0.110661 (* 1 = 0.110661 loss)
I0429 22:33:37.092267 16166 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I0429 22:33:38.013164 16166 solver.cpp:339] Iteration 400, Testing net (#0)
I0429 22:33:38.170788 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9764
I0429 22:33:38.170842 16166 solver.cpp:406]     Test net output #1: loss = 0.083706 (* 1 = 0.083706 loss)
I0429 22:33:38.173954 16166 solver.cpp:229] Iteration 400, loss = 0.130249
I0429 22:33:38.173985 16166 solver.cpp:245]     Train net output #0: loss = 0.130249 (* 1 = 0.130249 loss)
I0429 22:33:38.173998 16166 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I0429 22:33:39.102926 16166 solver.cpp:229] Iteration 500, loss = 0.110707
I0429 22:33:39.102982 16166 solver.cpp:245]     Train net output #0: loss = 0.110707 (* 1 = 0.110707 loss)
I0429 22:33:39.102994 16166 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I0429 22:33:40.019716 16166 solver.cpp:339] Iteration 600, Testing net (#0)
I0429 22:33:40.176959 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9792
I0429 22:33:40.177017 16166 solver.cpp:406]     Test net output #1: loss = 0.0594995 (* 1 = 0.0594995 loss)
I0429 22:33:40.180083 16166 solver.cpp:229] Iteration 600, loss = 0.064979
I0429 22:33:40.180112 16166 solver.cpp:245]     Train net output #0: loss = 0.0649791 (* 1 = 0.0649791 loss)
I0429 22:33:40.180124 16166 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I0429 22:33:41.105036 16166 solver.cpp:229] Iteration 700, loss = 0.0573017
I0429 22:33:41.105094 16166 solver.cpp:245]     Train net output #0: loss = 0.0573017 (* 1 = 0.0573017 loss)
I0429 22:33:41.105104 16166 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I0429 22:33:42.021003 16166 solver.cpp:339] Iteration 800, Testing net (#0)
I0429 22:33:42.178181 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9842
I0429 22:33:42.178231 16166 solver.cpp:406]     Test net output #1: loss = 0.0521032 (* 1 = 0.0521032 loss)
I0429 22:33:42.181298 16166 solver.cpp:229] Iteration 800, loss = 0.0440521
I0429 22:33:42.181329 16166 solver.cpp:245]     Train net output #0: loss = 0.0440521 (* 1 = 0.0440521 loss)
I0429 22:33:42.181341 16166 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I0429 22:33:43.103343 16166 solver.cpp:229] Iteration 900, loss = 0.0829805
I0429 22:33:43.103397 16166 solver.cpp:245]     Train net output #0: loss = 0.0829805 (* 1 = 0.0829805 loss)
I0429 22:33:43.103413 16166 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I0429 22:33:44.018668 16166 solver.cpp:339] Iteration 1000, Testing net (#0)
I0429 22:33:44.176475 16166 solver.cpp:406]     Test net output #0: accuracy = 0.982
I0429 22:33:44.176525 16166 solver.cpp:406]     Test net output #1: loss = 0.0544605 (* 1 = 0.0544605 loss)
I0429 22:33:44.179607 16166 solver.cpp:229] Iteration 1000, loss = 0.0414417
I0429 22:33:44.179637 16166 solver.cpp:245]     Train net output #0: loss = 0.0414417 (* 1 = 0.0414417 loss)
I0429 22:33:44.179651 16166 sgd_solver.cpp:105] Iteration 1000, lr = 0.00931012
I0429 22:33:45.109470 16166 solver.cpp:229] Iteration 1100, loss = 0.0461027
I0429 22:33:45.109524 16166 solver.cpp:245]     Train net output #0: loss = 0.0461027 (* 1 = 0.0461027 loss)
I0429 22:33:45.109534 16166 sgd_solver.cpp:105] Iteration 1100, lr = 0.00924715
I0429 22:33:46.022905 16166 solver.cpp:339] Iteration 1200, Testing net (#0)
I0429 22:33:46.181823 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9821
I0429 22:33:46.181880 16166 solver.cpp:406]     Test net output #1: loss = 0.0574968 (* 1 = 0.0574968 loss)
I0429 22:33:46.184954 16166 solver.cpp:229] Iteration 1200, loss = 0.045321
I0429 22:33:46.185000 16166 solver.cpp:245]     Train net output #0: loss = 0.045321 (* 1 = 0.045321 loss)
I0429 22:33:46.185015 16166 sgd_solver.cpp:105] Iteration 1200, lr = 0.00918515
I0429 22:33:47.107337 16166 solver.cpp:229] Iteration 1300, loss = 0.0336108
I0429 22:33:47.107393 16166 solver.cpp:245]     Train net output #0: loss = 0.0336108 (* 1 = 0.0336108 loss)
I0429 22:33:47.107403 16166 sgd_solver.cpp:105] Iteration 1300, lr = 0.00912412
I0429 22:33:48.023644 16166 solver.cpp:339] Iteration 1400, Testing net (#0)
I0429 22:33:48.181005 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9864
I0429 22:33:48.181052 16166 solver.cpp:406]     Test net output #1: loss = 0.0434186 (* 1 = 0.0434186 loss)
I0429 22:33:48.184123 16166 solver.cpp:229] Iteration 1400, loss = 0.00696874
I0429 22:33:48.184152 16166 solver.cpp:245]     Train net output #0: loss = 0.00696874 (* 1 = 0.00696874 loss)
I0429 22:33:48.184165 16166 sgd_solver.cpp:105] Iteration 1400, lr = 0.00906403
I0429 22:33:49.109340 16166 solver.cpp:229] Iteration 1500, loss = 0.0197318
I0429 22:33:49.109391 16166 solver.cpp:245]     Train net output #0: loss = 0.0197318 (* 1 = 0.0197318 loss)
I0429 22:33:49.109402 16166 sgd_solver.cpp:105] Iteration 1500, lr = 0.00900485
I0429 22:33:50.025985 16166 solver.cpp:339] Iteration 1600, Testing net (#0)
I0429 22:33:50.183359 16166 solver.cpp:406]     Test net output #0: accuracy = 0.987
I0429 22:33:50.183419 16166 solver.cpp:406]     Test net output #1: loss = 0.0405259 (* 1 = 0.0405259 loss)
I0429 22:33:50.186486 16166 solver.cpp:229] Iteration 1600, loss = 0.0223136
I0429 22:33:50.186514 16166 solver.cpp:245]     Train net output #0: loss = 0.0223136 (* 1 = 0.0223136 loss)
I0429 22:33:50.186527 16166 sgd_solver.cpp:105] Iteration 1600, lr = 0.00894657
I0429 22:33:51.112738 16166 solver.cpp:229] Iteration 1700, loss = 0.03093
I0429 22:33:51.112794 16166 solver.cpp:245]     Train net output #0: loss = 0.03093 (* 1 = 0.03093 loss)
I0429 22:33:51.112807 16166 sgd_solver.cpp:105] Iteration 1700, lr = 0.00888916
I0429 22:33:52.028920 16166 solver.cpp:339] Iteration 1800, Testing net (#0)
I0429 22:33:52.186378 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9875
I0429 22:33:52.186427 16166 solver.cpp:406]     Test net output #1: loss = 0.039348 (* 1 = 0.039348 loss)
I0429 22:33:52.189641 16166 solver.cpp:229] Iteration 1800, loss = 0.0287656
I0429 22:33:52.189671 16166 solver.cpp:245]     Train net output #0: loss = 0.0287656 (* 1 = 0.0287656 loss)
I0429 22:33:52.189683 16166 sgd_solver.cpp:105] Iteration 1800, lr = 0.0088326
I0429 22:33:53.115087 16166 solver.cpp:229] Iteration 1900, loss = 0.0206588
I0429 22:33:53.115139 16166 solver.cpp:245]     Train net output #0: loss = 0.0206588 (* 1 = 0.0206588 loss)
I0429 22:33:53.115149 16166 sgd_solver.cpp:105] Iteration 1900, lr = 0.00877687
I0429 22:33:54.030905 16166 solver.cpp:339] Iteration 2000, Testing net (#0)
I0429 22:33:54.187847 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9871
I0429 22:33:54.187898 16166 solver.cpp:406]     Test net output #1: loss = 0.0413509 (* 1 = 0.0413509 loss)
I0429 22:33:54.190960 16166 solver.cpp:229] Iteration 2000, loss = 0.0209031
I0429 22:33:54.190989 16166 solver.cpp:245]     Train net output #0: loss = 0.0209031 (* 1 = 0.0209031 loss)
I0429 22:33:54.191001 16166 sgd_solver.cpp:105] Iteration 2000, lr = 0.00872196
I0429 22:33:55.154510 16166 solver.cpp:229] Iteration 2100, loss = 0.0442123
I0429 22:33:55.154568 16166 solver.cpp:245]     Train net output #0: loss = 0.0442123 (* 1 = 0.0442123 loss)
I0429 22:33:55.154579 16166 sgd_solver.cpp:105] Iteration 2100, lr = 0.00866784
I0429 22:33:56.071208 16166 solver.cpp:339] Iteration 2200, Testing net (#0)
I0429 22:33:56.228749 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9892
I0429 22:33:56.228802 16166 solver.cpp:406]     Test net output #1: loss = 0.0336552 (* 1 = 0.0336552 loss)
I0429 22:33:56.231876 16166 solver.cpp:229] Iteration 2200, loss = 0.00812616
I0429 22:33:56.231906 16166 solver.cpp:245]     Train net output #0: loss = 0.00812616 (* 1 = 0.00812616 loss)
I0429 22:33:56.231920 16166 sgd_solver.cpp:105] Iteration 2200, lr = 0.0086145
I0429 22:33:57.158104 16166 solver.cpp:229] Iteration 2300, loss = 0.0449166
I0429 22:33:57.158154 16166 solver.cpp:245]     Train net output #0: loss = 0.0449166 (* 1 = 0.0449166 loss)
I0429 22:33:57.158164 16166 sgd_solver.cpp:105] Iteration 2300, lr = 0.00856192
I0429 22:33:58.074363 16166 solver.cpp:339] Iteration 2400, Testing net (#0)
I0429 22:33:58.231784 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9874
I0429 22:33:58.231834 16166 solver.cpp:406]     Test net output #1: loss = 0.0396115 (* 1 = 0.0396115 loss)
I0429 22:33:58.234891 16166 solver.cpp:229] Iteration 2400, loss = 0.0231164
I0429 22:33:58.234920 16166 solver.cpp:245]     Train net output #0: loss = 0.0231164 (* 1 = 0.0231164 loss)
I0429 22:33:58.234932 16166 sgd_solver.cpp:105] Iteration 2400, lr = 0.00851008
I0429 22:33:59.160251 16166 solver.cpp:229] Iteration 2500, loss = 0.0190849
I0429 22:33:59.160302 16166 solver.cpp:245]     Train net output #0: loss = 0.0190849 (* 1 = 0.0190849 loss)
I0429 22:33:59.160313 16166 sgd_solver.cpp:105] Iteration 2500, lr = 0.00845897
I0429 22:34:00.077431 16166 solver.cpp:339] Iteration 2600, Testing net (#0)
I0429 22:34:00.234586 16166 solver.cpp:406]     Test net output #0: accuracy = 0.988
I0429 22:34:00.234699 16166 solver.cpp:406]     Test net output #1: loss = 0.0350254 (* 1 = 0.0350254 loss)
I0429 22:34:00.237732 16166 solver.cpp:229] Iteration 2600, loss = 0.0241859
I0429 22:34:00.237762 16166 solver.cpp:245]     Train net output #0: loss = 0.0241859 (* 1 = 0.0241859 loss)
I0429 22:34:00.237776 16166 sgd_solver.cpp:105] Iteration 2600, lr = 0.00840857
I0429 22:34:01.163949 16166 solver.cpp:229] Iteration 2700, loss = 0.0193545
I0429 22:34:01.164005 16166 solver.cpp:245]     Train net output #0: loss = 0.0193545 (* 1 = 0.0193545 loss)
I0429 22:34:01.164016 16166 sgd_solver.cpp:105] Iteration 2700, lr = 0.00835886
I0429 22:34:02.081679 16166 solver.cpp:339] Iteration 2800, Testing net (#0)
I0429 22:34:02.239022 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9868
I0429 22:34:02.239073 16166 solver.cpp:406]     Test net output #1: loss = 0.038953 (* 1 = 0.038953 loss)
I0429 22:34:02.242074 16166 solver.cpp:229] Iteration 2800, loss = 0.0184035
I0429 22:34:02.242105 16166 solver.cpp:245]     Train net output #0: loss = 0.0184035 (* 1 = 0.0184035 loss)
I0429 22:34:02.242118 16166 sgd_solver.cpp:105] Iteration 2800, lr = 0.00830984
I0429 22:34:03.167320 16166 solver.cpp:229] Iteration 2900, loss = 0.0215849
I0429 22:34:03.168294 16166 solver.cpp:245]     Train net output #0: loss = 0.0215849 (* 1 = 0.0215849 loss)
I0429 22:34:03.168323 16166 sgd_solver.cpp:105] Iteration 2900, lr = 0.00826148
I0429 22:34:04.086956 16166 solver.cpp:339] Iteration 3000, Testing net (#0)
I0429 22:34:04.245122 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9874
I0429 22:34:04.245173 16166 solver.cpp:406]     Test net output #1: loss = 0.0368674 (* 1 = 0.0368674 loss)
I0429 22:34:04.248200 16166 solver.cpp:229] Iteration 3000, loss = 0.0095604
I0429 22:34:04.248234 16166 solver.cpp:245]     Train net output #0: loss = 0.00956038 (* 1 = 0.00956038 loss)
I0429 22:34:04.248247 16166 sgd_solver.cpp:105] Iteration 3000, lr = 0.00821377
I0429 22:34:05.176213 16166 solver.cpp:229] Iteration 3100, loss = 0.0107579
I0429 22:34:05.176267 16166 solver.cpp:245]     Train net output #0: loss = 0.0107579 (* 1 = 0.0107579 loss)
I0429 22:34:05.176278 16166 sgd_solver.cpp:105] Iteration 3100, lr = 0.0081667
I0429 22:34:06.094526 16166 solver.cpp:339] Iteration 3200, Testing net (#0)
I0429 22:34:06.252629 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9891
I0429 22:34:06.252682 16166 solver.cpp:406]     Test net output #1: loss = 0.0317178 (* 1 = 0.0317178 loss)
I0429 22:34:06.255762 16166 solver.cpp:229] Iteration 3200, loss = 0.0439522
I0429 22:34:06.255791 16166 solver.cpp:245]     Train net output #0: loss = 0.0439522 (* 1 = 0.0439522 loss)
I0429 22:34:06.255805 16166 sgd_solver.cpp:105] Iteration 3200, lr = 0.00812025
I0429 22:34:07.184545 16166 solver.cpp:229] Iteration 3300, loss = 0.00793862
I0429 22:34:07.184597 16166 solver.cpp:245]     Train net output #0: loss = 0.00793861 (* 1 = 0.00793861 loss)
I0429 22:34:07.184607 16166 sgd_solver.cpp:105] Iteration 3300, lr = 0.00807442
I0429 22:34:08.103664 16166 solver.cpp:339] Iteration 3400, Testing net (#0)
I0429 22:34:08.262040 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9898
I0429 22:34:08.262089 16166 solver.cpp:406]     Test net output #1: loss = 0.0308138 (* 1 = 0.0308138 loss)
I0429 22:34:08.265141 16166 solver.cpp:229] Iteration 3400, loss = 0.00464675
I0429 22:34:08.265171 16166 solver.cpp:245]     Train net output #0: loss = 0.00464673 (* 1 = 0.00464673 loss)
I0429 22:34:08.265183 16166 sgd_solver.cpp:105] Iteration 3400, lr = 0.00802918
I0429 22:34:09.193262 16166 solver.cpp:229] Iteration 3500, loss = 0.00852241
I0429 22:34:09.193315 16166 solver.cpp:245]     Train net output #0: loss = 0.0085224 (* 1 = 0.0085224 loss)
I0429 22:34:09.193326 16166 sgd_solver.cpp:105] Iteration 3500, lr = 0.00798454
I0429 22:34:10.110864 16166 solver.cpp:339] Iteration 3600, Testing net (#0)
I0429 22:34:10.267868 16166 solver.cpp:406]     Test net output #0: accuracy = 0.991
I0429 22:34:10.267930 16166 solver.cpp:406]     Test net output #1: loss = 0.0304812 (* 1 = 0.0304812 loss)
I0429 22:34:10.271004 16166 solver.cpp:229] Iteration 3600, loss = 0.00526106
I0429 22:34:10.271035 16166 solver.cpp:245]     Train net output #0: loss = 0.00526105 (* 1 = 0.00526105 loss)
I0429 22:34:10.271047 16166 sgd_solver.cpp:105] Iteration 3600, lr = 0.00794046
I0429 22:34:11.197163 16166 solver.cpp:229] Iteration 3700, loss = 0.0241572
I0429 22:34:11.197221 16166 solver.cpp:245]     Train net output #0: loss = 0.0241572 (* 1 = 0.0241572 loss)
I0429 22:34:11.197232 16166 sgd_solver.cpp:105] Iteration 3700, lr = 0.00789695
I0429 22:34:12.114109 16166 solver.cpp:339] Iteration 3800, Testing net (#0)
I0429 22:34:12.271191 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9894
I0429 22:34:12.271237 16166 solver.cpp:406]     Test net output #1: loss = 0.0314388 (* 1 = 0.0314388 loss)
I0429 22:34:12.274229 16166 solver.cpp:229] Iteration 3800, loss = 0.012765
I0429 22:34:12.274258 16166 solver.cpp:245]     Train net output #0: loss = 0.012765 (* 1 = 0.012765 loss)
I0429 22:34:12.274269 16166 sgd_solver.cpp:105] Iteration 3800, lr = 0.007854
I0429 22:34:13.199626 16166 solver.cpp:229] Iteration 3900, loss = 0.0163184
I0429 22:34:13.199678 16166 solver.cpp:245]     Train net output #0: loss = 0.0163184 (* 1 = 0.0163184 loss)
I0429 22:34:13.199750 16166 sgd_solver.cpp:105] Iteration 3900, lr = 0.00781158
I0429 22:34:14.116225 16166 solver.cpp:339] Iteration 4000, Testing net (#0)
I0429 22:34:14.273655 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9897
I0429 22:34:14.273705 16166 solver.cpp:406]     Test net output #1: loss = 0.0325555 (* 1 = 0.0325555 loss)
I0429 22:34:14.276736 16166 solver.cpp:229] Iteration 4000, loss = 0.00570006
I0429 22:34:14.276765 16166 solver.cpp:245]     Train net output #0: loss = 0.00570006 (* 1 = 0.00570006 loss)
I0429 22:34:14.276777 16166 sgd_solver.cpp:105] Iteration 4000, lr = 0.0077697
I0429 22:34:15.203961 16166 solver.cpp:229] Iteration 4100, loss = 0.00758013
I0429 22:34:15.204011 16166 solver.cpp:245]     Train net output #0: loss = 0.00758013 (* 1 = 0.00758013 loss)
I0429 22:34:15.204022 16166 sgd_solver.cpp:105] Iteration 4100, lr = 0.00772833
I0429 22:34:16.119686 16166 solver.cpp:339] Iteration 4200, Testing net (#0)
I0429 22:34:16.277160 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9892
I0429 22:34:16.277211 16166 solver.cpp:406]     Test net output #1: loss = 0.0316687 (* 1 = 0.0316687 loss)
I0429 22:34:16.280258 16166 solver.cpp:229] Iteration 4200, loss = 0.0198809
I0429 22:34:16.280288 16166 solver.cpp:245]     Train net output #0: loss = 0.0198809 (* 1 = 0.0198809 loss)
I0429 22:34:16.280300 16166 sgd_solver.cpp:105] Iteration 4200, lr = 0.00768748
I0429 22:34:17.205265 16166 solver.cpp:229] Iteration 4300, loss = 0.00684102
I0429 22:34:17.205317 16166 solver.cpp:245]     Train net output #0: loss = 0.00684102 (* 1 = 0.00684102 loss)
I0429 22:34:17.205327 16166 sgd_solver.cpp:105] Iteration 4300, lr = 0.00764712
I0429 22:34:18.122354 16166 solver.cpp:339] Iteration 4400, Testing net (#0)
I0429 22:34:18.280369 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9891
I0429 22:34:18.280424 16166 solver.cpp:406]     Test net output #1: loss = 0.0322981 (* 1 = 0.0322981 loss)
I0429 22:34:18.283478 16166 solver.cpp:229] Iteration 4400, loss = 0.00815209
I0429 22:34:18.283509 16166 solver.cpp:245]     Train net output #0: loss = 0.00815209 (* 1 = 0.00815209 loss)
I0429 22:34:18.283522 16166 sgd_solver.cpp:105] Iteration 4400, lr = 0.00760726
I0429 22:34:19.209146 16166 solver.cpp:229] Iteration 4500, loss = 0.0101259
I0429 22:34:19.209198 16166 solver.cpp:245]     Train net output #0: loss = 0.0101259 (* 1 = 0.0101259 loss)
I0429 22:34:19.209209 16166 sgd_solver.cpp:105] Iteration 4500, lr = 0.00756788
I0429 22:34:20.126003 16166 solver.cpp:339] Iteration 4600, Testing net (#0)
I0429 22:34:20.283907 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9897
I0429 22:34:20.283967 16166 solver.cpp:406]     Test net output #1: loss = 0.0294307 (* 1 = 0.0294307 loss)
I0429 22:34:20.287008 16166 solver.cpp:229] Iteration 4600, loss = 0.0149767
I0429 22:34:20.287037 16166 solver.cpp:245]     Train net output #0: loss = 0.0149767 (* 1 = 0.0149767 loss)
I0429 22:34:20.287050 16166 sgd_solver.cpp:105] Iteration 4600, lr = 0.00752897
I0429 22:34:21.211439 16166 solver.cpp:229] Iteration 4700, loss = 0.00440547
I0429 22:34:21.211496 16166 solver.cpp:245]     Train net output #0: loss = 0.00440546 (* 1 = 0.00440546 loss)
I0429 22:34:21.211506 16166 sgd_solver.cpp:105] Iteration 4700, lr = 0.00749052
I0429 22:34:22.129158 16166 solver.cpp:339] Iteration 4800, Testing net (#0)
I0429 22:34:22.286340 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9888
I0429 22:34:22.286391 16166 solver.cpp:406]     Test net output #1: loss = 0.0315056 (* 1 = 0.0315056 loss)
I0429 22:34:22.289434 16166 solver.cpp:229] Iteration 4800, loss = 0.00909472
I0429 22:34:22.289464 16166 solver.cpp:245]     Train net output #0: loss = 0.00909471 (* 1 = 0.00909471 loss)
I0429 22:34:22.289476 16166 sgd_solver.cpp:105] Iteration 4800, lr = 0.00745253
I0429 22:34:23.215664 16166 solver.cpp:229] Iteration 4900, loss = 0.010204
I0429 22:34:23.215716 16166 solver.cpp:245]     Train net output #0: loss = 0.010204 (* 1 = 0.010204 loss)
I0429 22:34:23.215726 16166 sgd_solver.cpp:105] Iteration 4900, lr = 0.00741498
I0429 22:34:24.131681 16166 solver.cpp:456] Snapshotting to binary proto file examples/mnist/lenet_iter_5000.caffemodel
I0429 22:34:24.153388 16166 sgd_solver.cpp:272] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_5000.solverstate
I0429 22:34:24.159096 16166 solver.cpp:339] Iteration 5000, Testing net (#0)
I0429 22:34:24.310940 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9903
I0429 22:34:24.310999 16166 solver.cpp:406]     Test net output #1: loss = 0.0300159 (* 1 = 0.0300159 loss)
I0429 22:34:24.314203 16166 solver.cpp:229] Iteration 5000, loss = 0.0098476
I0429 22:34:24.314260 16166 solver.cpp:245]     Train net output #0: loss = 0.00984759 (* 1 = 0.00984759 loss)
I0429 22:34:24.314275 16166 sgd_solver.cpp:105] Iteration 5000, lr = 0.00737788
I0429 22:34:25.246038 16166 solver.cpp:229] Iteration 5100, loss = 0.00783703
I0429 22:34:25.246093 16166 solver.cpp:245]     Train net output #0: loss = 0.00783702 (* 1 = 0.00783702 loss)
I0429 22:34:25.246105 16166 sgd_solver.cpp:105] Iteration 5100, lr = 0.0073412
I0429 22:34:26.165727 16166 solver.cpp:339] Iteration 5200, Testing net (#0)
I0429 22:34:26.324283 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9888
I0429 22:34:26.324339 16166 solver.cpp:406]     Test net output #1: loss = 0.034868 (* 1 = 0.034868 loss)
I0429 22:34:26.327425 16166 solver.cpp:229] Iteration 5200, loss = 0.0115827
I0429 22:34:26.327456 16166 solver.cpp:245]     Train net output #0: loss = 0.0115827 (* 1 = 0.0115827 loss)
I0429 22:34:26.327468 16166 sgd_solver.cpp:105] Iteration 5200, lr = 0.00730495
I0429 22:34:27.253762 16166 solver.cpp:229] Iteration 5300, loss = 0.00753916
I0429 22:34:27.253815 16166 solver.cpp:245]     Train net output #0: loss = 0.00753916 (* 1 = 0.00753916 loss)
I0429 22:34:27.253826 16166 sgd_solver.cpp:105] Iteration 5300, lr = 0.00726911
I0429 22:34:28.172772 16166 solver.cpp:339] Iteration 5400, Testing net (#0)
I0429 22:34:28.330431 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9895
I0429 22:34:28.330485 16166 solver.cpp:406]     Test net output #1: loss = 0.0334766 (* 1 = 0.0334766 loss)
I0429 22:34:28.333606 16166 solver.cpp:229] Iteration 5400, loss = 0.00735724
I0429 22:34:28.333636 16166 solver.cpp:245]     Train net output #0: loss = 0.00735724 (* 1 = 0.00735724 loss)
I0429 22:34:28.333648 16166 sgd_solver.cpp:105] Iteration 5400, lr = 0.00723368
I0429 22:34:29.259913 16166 solver.cpp:229] Iteration 5500, loss = 0.00433688
I0429 22:34:29.259966 16166 solver.cpp:245]     Train net output #0: loss = 0.00433688 (* 1 = 0.00433688 loss)
I0429 22:34:29.259977 16166 sgd_solver.cpp:105] Iteration 5500, lr = 0.00719865
I0429 22:34:30.177700 16166 solver.cpp:339] Iteration 5600, Testing net (#0)
I0429 22:34:30.335273 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9904
I0429 22:34:30.335342 16166 solver.cpp:406]     Test net output #1: loss = 0.0290709 (* 1 = 0.0290709 loss)
I0429 22:34:30.338368 16166 solver.cpp:229] Iteration 5600, loss = 0.00799395
I0429 22:34:30.338398 16166 solver.cpp:245]     Train net output #0: loss = 0.00799394 (* 1 = 0.00799394 loss)
I0429 22:34:30.338412 16166 sgd_solver.cpp:105] Iteration 5600, lr = 0.00716402
I0429 22:34:31.265696 16166 solver.cpp:229] Iteration 5700, loss = 0.00456313
I0429 22:34:31.265746 16166 solver.cpp:245]     Train net output #0: loss = 0.00456313 (* 1 = 0.00456313 loss)
I0429 22:34:31.265758 16166 sgd_solver.cpp:105] Iteration 5700, lr = 0.00712977
I0429 22:34:32.183538 16166 solver.cpp:339] Iteration 5800, Testing net (#0)
I0429 22:34:32.341802 16166 solver.cpp:406]     Test net output #0: accuracy = 0.99
I0429 22:34:32.341855 16166 solver.cpp:406]     Test net output #1: loss = 0.030346 (* 1 = 0.030346 loss)
I0429 22:34:32.344888 16166 solver.cpp:229] Iteration 5800, loss = 0.00322317
I0429 22:34:32.344919 16166 solver.cpp:245]     Train net output #0: loss = 0.00322316 (* 1 = 0.00322316 loss)
I0429 22:34:32.344933 16166 sgd_solver.cpp:105] Iteration 5800, lr = 0.0070959
I0429 22:34:33.272176 16166 solver.cpp:229] Iteration 5900, loss = 0.000893828
I0429 22:34:33.272516 16166 solver.cpp:245]     Train net output #0: loss = 0.000893824 (* 1 = 0.000893824 loss)
I0429 22:34:33.272529 16166 sgd_solver.cpp:105] Iteration 5900, lr = 0.0070624
I0429 22:34:34.190397 16166 solver.cpp:339] Iteration 6000, Testing net (#0)
I0429 22:34:34.347985 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9904
I0429 22:34:34.348037 16166 solver.cpp:406]     Test net output #1: loss = 0.029414 (* 1 = 0.029414 loss)
I0429 22:34:34.351075 16166 solver.cpp:229] Iteration 6000, loss = 0.0118149
I0429 22:34:34.351104 16166 solver.cpp:245]     Train net output #0: loss = 0.0118149 (* 1 = 0.0118149 loss)
I0429 22:34:34.351117 16166 sgd_solver.cpp:105] Iteration 6000, lr = 0.00702927
I0429 22:34:35.282858 16166 solver.cpp:229] Iteration 6100, loss = 0.00729991
I0429 22:34:35.282914 16166 solver.cpp:245]     Train net output #0: loss = 0.00729991 (* 1 = 0.00729991 loss)
I0429 22:34:35.282925 16166 sgd_solver.cpp:105] Iteration 6100, lr = 0.0069965
I0429 22:34:36.200774 16166 solver.cpp:339] Iteration 6200, Testing net (#0)
I0429 22:34:36.358285 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9905
I0429 22:34:36.358338 16166 solver.cpp:406]     Test net output #1: loss = 0.0293044 (* 1 = 0.0293044 loss)
I0429 22:34:36.361372 16166 solver.cpp:229] Iteration 6200, loss = 0.0060972
I0429 22:34:36.361402 16166 solver.cpp:245]     Train net output #0: loss = 0.00609719 (* 1 = 0.00609719 loss)
I0429 22:34:36.361415 16166 sgd_solver.cpp:105] Iteration 6200, lr = 0.00696408
I0429 22:34:37.288444 16166 solver.cpp:229] Iteration 6300, loss = 0.0187763
I0429 22:34:37.288497 16166 solver.cpp:245]     Train net output #0: loss = 0.0187763 (* 1 = 0.0187763 loss)
I0429 22:34:37.288508 16166 sgd_solver.cpp:105] Iteration 6300, lr = 0.00693201
I0429 22:34:38.206147 16166 solver.cpp:339] Iteration 6400, Testing net (#0)
I0429 22:34:38.364133 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9904
I0429 22:34:38.364187 16166 solver.cpp:406]     Test net output #1: loss = 0.0308344 (* 1 = 0.0308344 loss)
I0429 22:34:38.367233 16166 solver.cpp:229] Iteration 6400, loss = 0.00292841
I0429 22:34:38.367264 16166 solver.cpp:245]     Train net output #0: loss = 0.00292841 (* 1 = 0.00292841 loss)
I0429 22:34:38.367276 16166 sgd_solver.cpp:105] Iteration 6400, lr = 0.00690029
I0429 22:34:39.294134 16166 solver.cpp:229] Iteration 6500, loss = 0.00483967
I0429 22:34:39.294193 16166 solver.cpp:245]     Train net output #0: loss = 0.00483966 (* 1 = 0.00483966 loss)
I0429 22:34:39.294204 16166 sgd_solver.cpp:105] Iteration 6500, lr = 0.0068689
I0429 22:34:40.212760 16166 solver.cpp:339] Iteration 6600, Testing net (#0)
I0429 22:34:40.370604 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9887
I0429 22:34:40.370666 16166 solver.cpp:406]     Test net output #1: loss = 0.0335886 (* 1 = 0.0335886 loss)
I0429 22:34:40.373685 16166 solver.cpp:229] Iteration 6600, loss = 0.00229603
I0429 22:34:40.373716 16166 solver.cpp:245]     Train net output #0: loss = 0.00229602 (* 1 = 0.00229602 loss)
I0429 22:34:40.373728 16166 sgd_solver.cpp:105] Iteration 6600, lr = 0.00683784
I0429 22:34:41.300480 16166 solver.cpp:229] Iteration 6700, loss = 0.00822386
I0429 22:34:41.300535 16166 solver.cpp:245]     Train net output #0: loss = 0.00822385 (* 1 = 0.00822385 loss)
I0429 22:34:41.300546 16166 sgd_solver.cpp:105] Iteration 6700, lr = 0.00680711
I0429 22:34:42.218201 16166 solver.cpp:339] Iteration 6800, Testing net (#0)
I0429 22:34:42.376153 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9897
I0429 22:34:42.376214 16166 solver.cpp:406]     Test net output #1: loss = 0.032111 (* 1 = 0.032111 loss)
I0429 22:34:42.379393 16166 solver.cpp:229] Iteration 6800, loss = 0.00548693
I0429 22:34:42.379423 16166 solver.cpp:245]     Train net output #0: loss = 0.00548692 (* 1 = 0.00548692 loss)
I0429 22:34:42.379436 16166 sgd_solver.cpp:105] Iteration 6800, lr = 0.0067767
I0429 22:34:43.310261 16166 solver.cpp:229] Iteration 6900, loss = 0.0274155
I0429 22:34:43.310323 16166 solver.cpp:245]     Train net output #0: loss = 0.0274155 (* 1 = 0.0274155 loss)
I0429 22:34:43.310406 16166 sgd_solver.cpp:105] Iteration 6900, lr = 0.0067466
I0429 22:34:44.230620 16166 solver.cpp:339] Iteration 7000, Testing net (#0)
I0429 22:34:44.390117 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9909
I0429 22:34:44.390168 16166 solver.cpp:406]     Test net output #1: loss = 0.0293249 (* 1 = 0.0293249 loss)
I0429 22:34:44.393288 16166 solver.cpp:229] Iteration 7000, loss = 0.00557634
I0429 22:34:44.393318 16166 solver.cpp:245]     Train net output #0: loss = 0.00557633 (* 1 = 0.00557633 loss)
I0429 22:34:44.393332 16166 sgd_solver.cpp:105] Iteration 7000, lr = 0.00671681
I0429 22:34:45.323653 16166 solver.cpp:229] Iteration 7100, loss = 0.00697329
I0429 22:34:45.323711 16166 solver.cpp:245]     Train net output #0: loss = 0.00697329 (* 1 = 0.00697329 loss)
I0429 22:34:45.323731 16166 sgd_solver.cpp:105] Iteration 7100, lr = 0.00668733
I0429 22:34:46.240211 16166 solver.cpp:339] Iteration 7200, Testing net (#0)
I0429 22:34:46.397696 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9903
I0429 22:34:46.397750 16166 solver.cpp:406]     Test net output #1: loss = 0.0295844 (* 1 = 0.0295844 loss)
I0429 22:34:46.400876 16166 solver.cpp:229] Iteration 7200, loss = 0.0151028
I0429 22:34:46.400904 16166 solver.cpp:245]     Train net output #0: loss = 0.0151028 (* 1 = 0.0151028 loss)
I0429 22:34:46.400918 16166 sgd_solver.cpp:105] Iteration 7200, lr = 0.00665815
I0429 22:34:47.325803 16166 solver.cpp:229] Iteration 7300, loss = 0.00433811
I0429 22:34:47.325860 16166 solver.cpp:245]     Train net output #0: loss = 0.00433811 (* 1 = 0.00433811 loss)
I0429 22:34:47.325871 16166 sgd_solver.cpp:105] Iteration 7300, lr = 0.00662927
I0429 22:34:48.238888 16166 solver.cpp:339] Iteration 7400, Testing net (#0)
I0429 22:34:48.396937 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9903
I0429 22:34:48.396993 16166 solver.cpp:406]     Test net output #1: loss = 0.0302007 (* 1 = 0.0302007 loss)
I0429 22:34:48.400007 16166 solver.cpp:229] Iteration 7400, loss = 0.0053707
I0429 22:34:48.400035 16166 solver.cpp:245]     Train net output #0: loss = 0.0053707 (* 1 = 0.0053707 loss)
I0429 22:34:48.400048 16166 sgd_solver.cpp:105] Iteration 7400, lr = 0.00660067
I0429 22:34:49.325670 16166 solver.cpp:229] Iteration 7500, loss = 0.00877948
I0429 22:34:49.325722 16166 solver.cpp:245]     Train net output #0: loss = 0.00877948 (* 1 = 0.00877948 loss)
I0429 22:34:49.325733 16166 sgd_solver.cpp:105] Iteration 7500, lr = 0.00657236
I0429 22:34:50.242141 16166 solver.cpp:339] Iteration 7600, Testing net (#0)
I0429 22:34:50.399729 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9906
I0429 22:34:50.399791 16166 solver.cpp:406]     Test net output #1: loss = 0.0288383 (* 1 = 0.0288383 loss)
I0429 22:34:50.402850 16166 solver.cpp:229] Iteration 7600, loss = 0.00794104
I0429 22:34:50.402880 16166 solver.cpp:245]     Train net output #0: loss = 0.00794104 (* 1 = 0.00794104 loss)
I0429 22:34:50.402894 16166 sgd_solver.cpp:105] Iteration 7600, lr = 0.00654433
I0429 22:34:51.329165 16166 solver.cpp:229] Iteration 7700, loss = 0.0189114
I0429 22:34:51.329221 16166 solver.cpp:245]     Train net output #0: loss = 0.0189114 (* 1 = 0.0189114 loss)
I0429 22:34:51.329231 16166 sgd_solver.cpp:105] Iteration 7700, lr = 0.00651658
I0429 22:34:52.246516 16166 solver.cpp:339] Iteration 7800, Testing net (#0)
I0429 22:34:52.404240 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9906
I0429 22:34:52.404292 16166 solver.cpp:406]     Test net output #1: loss = 0.0300127 (* 1 = 0.0300127 loss)
I0429 22:34:52.407327 16166 solver.cpp:229] Iteration 7800, loss = 0.00284821
I0429 22:34:52.407357 16166 solver.cpp:245]     Train net output #0: loss = 0.0028482 (* 1 = 0.0028482 loss)
I0429 22:34:52.407369 16166 sgd_solver.cpp:105] Iteration 7800, lr = 0.00648911
I0429 22:34:53.333562 16166 solver.cpp:229] Iteration 7900, loss = 0.00639338
I0429 22:34:53.333614 16166 solver.cpp:245]     Train net output #0: loss = 0.00639338 (* 1 = 0.00639338 loss)
I0429 22:34:53.333626 16166 sgd_solver.cpp:105] Iteration 7900, lr = 0.0064619
I0429 22:34:54.250007 16166 solver.cpp:339] Iteration 8000, Testing net (#0)
I0429 22:34:54.407661 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9895
I0429 22:34:54.407716 16166 solver.cpp:406]     Test net output #1: loss = 0.0309863 (* 1 = 0.0309863 loss)
I0429 22:34:54.410742 16166 solver.cpp:229] Iteration 8000, loss = 0.00733254
I0429 22:34:54.410770 16166 solver.cpp:245]     Train net output #0: loss = 0.00733254 (* 1 = 0.00733254 loss)
I0429 22:34:54.410784 16166 sgd_solver.cpp:105] Iteration 8000, lr = 0.00643496
I0429 22:34:55.339646 16166 solver.cpp:229] Iteration 8100, loss = 0.00319268
I0429 22:34:55.339704 16166 solver.cpp:245]     Train net output #0: loss = 0.00319267 (* 1 = 0.00319267 loss)
I0429 22:34:55.339715 16166 sgd_solver.cpp:105] Iteration 8100, lr = 0.00640827
I0429 22:34:56.256033 16166 solver.cpp:339] Iteration 8200, Testing net (#0)
I0429 22:34:56.414477 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9896
I0429 22:34:56.414530 16166 solver.cpp:406]     Test net output #1: loss = 0.0322045 (* 1 = 0.0322045 loss)
I0429 22:34:56.417569 16166 solver.cpp:229] Iteration 8200, loss = 0.00111208
I0429 22:34:56.417599 16166 solver.cpp:245]     Train net output #0: loss = 0.00111207 (* 1 = 0.00111207 loss)
I0429 22:34:56.417613 16166 sgd_solver.cpp:105] Iteration 8200, lr = 0.00638185
I0429 22:34:57.344367 16166 solver.cpp:229] Iteration 8300, loss = 0.00323125
I0429 22:34:57.344421 16166 solver.cpp:245]     Train net output #0: loss = 0.00323124 (* 1 = 0.00323124 loss)
I0429 22:34:57.344434 16166 sgd_solver.cpp:105] Iteration 8300, lr = 0.00635567
I0429 22:34:58.261440 16166 solver.cpp:339] Iteration 8400, Testing net (#0)
I0429 22:34:58.418299 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9903
I0429 22:34:58.418351 16166 solver.cpp:406]     Test net output #1: loss = 0.0304889 (* 1 = 0.0304889 loss)
I0429 22:34:58.421386 16166 solver.cpp:229] Iteration 8400, loss = 0.00712645
I0429 22:34:58.421414 16166 solver.cpp:245]     Train net output #0: loss = 0.00712645 (* 1 = 0.00712645 loss)
I0429 22:34:58.421427 16166 sgd_solver.cpp:105] Iteration 8400, lr = 0.00632975
I0429 22:34:59.347825 16166 solver.cpp:229] Iteration 8500, loss = 0.00882526
I0429 22:34:59.347878 16166 solver.cpp:245]     Train net output #0: loss = 0.00882525 (* 1 = 0.00882525 loss)
I0429 22:34:59.347889 16166 sgd_solver.cpp:105] Iteration 8500, lr = 0.00630407
I0429 22:35:00.265092 16166 solver.cpp:339] Iteration 8600, Testing net (#0)
I0429 22:35:00.423046 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9905
I0429 22:35:00.423108 16166 solver.cpp:406]     Test net output #1: loss = 0.0290956 (* 1 = 0.0290956 loss)
I0429 22:35:00.426163 16166 solver.cpp:229] Iteration 8600, loss = 0.00466248
I0429 22:35:00.426193 16166 solver.cpp:245]     Train net output #0: loss = 0.00466247 (* 1 = 0.00466247 loss)
I0429 22:35:00.426206 16166 sgd_solver.cpp:105] Iteration 8600, lr = 0.00627864
I0429 22:35:01.350917 16166 solver.cpp:229] Iteration 8700, loss = 0.00821988
I0429 22:35:01.350970 16166 solver.cpp:245]     Train net output #0: loss = 0.00821987 (* 1 = 0.00821987 loss)
I0429 22:35:01.350981 16166 sgd_solver.cpp:105] Iteration 8700, lr = 0.00625344
I0429 22:35:02.267010 16166 solver.cpp:339] Iteration 8800, Testing net (#0)
I0429 22:35:02.424590 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9899
I0429 22:35:02.424641 16166 solver.cpp:406]     Test net output #1: loss = 0.029083 (* 1 = 0.029083 loss)
I0429 22:35:02.427709 16166 solver.cpp:229] Iteration 8800, loss = 0.00267336
I0429 22:35:02.427739 16166 solver.cpp:245]     Train net output #0: loss = 0.00267335 (* 1 = 0.00267335 loss)
I0429 22:35:02.427752 16166 sgd_solver.cpp:105] Iteration 8800, lr = 0.00622847
I0429 22:35:03.350488 16166 solver.cpp:229] Iteration 8900, loss = 0.00246543
I0429 22:35:03.350862 16166 solver.cpp:245]     Train net output #0: loss = 0.00246542 (* 1 = 0.00246542 loss)
I0429 22:35:03.350875 16166 sgd_solver.cpp:105] Iteration 8900, lr = 0.00620374
I0429 22:35:04.264580 16166 solver.cpp:339] Iteration 9000, Testing net (#0)
I0429 22:35:04.421954 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9899
I0429 22:35:04.422006 16166 solver.cpp:406]     Test net output #1: loss = 0.0285256 (* 1 = 0.0285256 loss)
I0429 22:35:04.425083 16166 solver.cpp:229] Iteration 9000, loss = 0.00286962
I0429 22:35:04.425112 16166 solver.cpp:245]     Train net output #0: loss = 0.00286961 (* 1 = 0.00286961 loss)
I0429 22:35:04.425125 16166 sgd_solver.cpp:105] Iteration 9000, lr = 0.00617924
I0429 22:35:05.353008 16166 solver.cpp:229] Iteration 9100, loss = 0.00381211
I0429 22:35:05.353063 16166 solver.cpp:245]     Train net output #0: loss = 0.00381211 (* 1 = 0.00381211 loss)
I0429 22:35:05.353076 16166 sgd_solver.cpp:105] Iteration 9100, lr = 0.00615496
I0429 22:35:06.267846 16166 solver.cpp:339] Iteration 9200, Testing net (#0)
I0429 22:35:06.425884 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9902
I0429 22:35:06.425937 16166 solver.cpp:406]     Test net output #1: loss = 0.031205 (* 1 = 0.031205 loss)
I0429 22:35:06.428990 16166 solver.cpp:229] Iteration 9200, loss = 0.00401114
I0429 22:35:06.429020 16166 solver.cpp:245]     Train net output #0: loss = 0.00401113 (* 1 = 0.00401113 loss)
I0429 22:35:06.429033 16166 sgd_solver.cpp:105] Iteration 9200, lr = 0.0061309
I0429 22:35:07.351855 16166 solver.cpp:229] Iteration 9300, loss = 0.00466407
I0429 22:35:07.351909 16166 solver.cpp:245]     Train net output #0: loss = 0.00466406 (* 1 = 0.00466406 loss)
I0429 22:35:07.351920 16166 sgd_solver.cpp:105] Iteration 9300, lr = 0.00610706
I0429 22:35:08.268232 16166 solver.cpp:339] Iteration 9400, Testing net (#0)
I0429 22:35:08.426529 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9905
I0429 22:35:08.426583 16166 solver.cpp:406]     Test net output #1: loss = 0.0298795 (* 1 = 0.0298795 loss)
I0429 22:35:08.429615 16166 solver.cpp:229] Iteration 9400, loss = 0.00161929
I0429 22:35:08.429646 16166 solver.cpp:245]     Train net output #0: loss = 0.00161928 (* 1 = 0.00161928 loss)
I0429 22:35:08.429658 16166 sgd_solver.cpp:105] Iteration 9400, lr = 0.00608343
I0429 22:35:09.356359 16166 solver.cpp:229] Iteration 9500, loss = 0.00312845
I0429 22:35:09.356431 16166 solver.cpp:245]     Train net output #0: loss = 0.00312844 (* 1 = 0.00312844 loss)
I0429 22:35:09.356441 16166 sgd_solver.cpp:105] Iteration 9500, lr = 0.00606002
I0429 22:35:10.274809 16166 solver.cpp:339] Iteration 9600, Testing net (#0)
I0429 22:35:10.433612 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9899
I0429 22:35:10.433673 16166 solver.cpp:406]     Test net output #1: loss = 0.030839 (* 1 = 0.030839 loss)
I0429 22:35:10.436779 16166 solver.cpp:229] Iteration 9600, loss = 0.00700894
I0429 22:35:10.436812 16166 solver.cpp:245]     Train net output #0: loss = 0.00700893 (* 1 = 0.00700893 loss)
I0429 22:35:10.436826 16166 sgd_solver.cpp:105] Iteration 9600, lr = 0.00603682
I0429 22:35:11.364167 16166 solver.cpp:229] Iteration 9700, loss = 0.00101664
I0429 22:35:11.364225 16166 solver.cpp:245]     Train net output #0: loss = 0.00101663 (* 1 = 0.00101663 loss)
I0429 22:35:11.364236 16166 sgd_solver.cpp:105] Iteration 9700, lr = 0.00601382
I0429 22:35:12.281468 16166 solver.cpp:339] Iteration 9800, Testing net (#0)
I0429 22:35:12.439178 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9901
I0429 22:35:12.439231 16166 solver.cpp:406]     Test net output #1: loss = 0.029355 (* 1 = 0.029355 loss)
I0429 22:35:12.442279 16166 solver.cpp:229] Iteration 9800, loss = 0.00865835
I0429 22:35:12.442308 16166 solver.cpp:245]     Train net output #0: loss = 0.00865834 (* 1 = 0.00865834 loss)
I0429 22:35:12.442322 16166 sgd_solver.cpp:105] Iteration 9800, lr = 0.00599102
I0429 22:35:13.367665 16166 solver.cpp:229] Iteration 9900, loss = 0.00493401
I0429 22:35:13.367723 16166 solver.cpp:245]     Train net output #0: loss = 0.004934 (* 1 = 0.004934 loss)
I0429 22:35:13.367805 16166 sgd_solver.cpp:105] Iteration 9900, lr = 0.00596843
I0429 22:35:14.284868 16166 solver.cpp:456] Snapshotting to binary proto file examples/mnist/lenet_iter_10000.caffemodel
I0429 22:35:14.303815 16166 sgd_solver.cpp:272] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10000.solverstate
I0429 22:35:14.312228 16166 solver.cpp:319] Iteration 10000, loss = 0.00286398
I0429 22:35:14.312265 16166 solver.cpp:339] Iteration 10000, Testing net (#0)
I0429 22:35:14.466413 16166 solver.cpp:406]     Test net output #0: accuracy = 0.9903
I0429 22:35:14.466471 16166 solver.cpp:406]     Test net output #1: loss = 0.0284385 (* 1 = 0.0284385 loss)
I0429 22:35:14.466480 16166 solver.cpp:324] Optimization Done.
I0429 22:35:14.466487 16166 caffe.cpp:222] Optimization Done.
